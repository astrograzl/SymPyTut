
%!TEX root = sympy_tutorial.tex




%=======================================================================  matrices
\section{Linear algebra}
\label{sec:linear_algebra}

%\ifthenelse{\boolean{FORLA}}{


\small
\begin{verbatimtab}
from sympy import Matrix
\end{verbatimtab}
\normalsize

\noindent
A matrix $A \in \mathbb{R}^{m\times n}$ is a rectangular array of real numbers with $m$ rows and $n$ columns.
To specify a matrix $A$, we specify the values for its $mn$ components $a_{11}, a_{12}, \ldots, a_{mn}$
as a list of lists:

\small
\begin{verbatimtab}
>>> A = Matrix( [[ 2,-3,-8, 7],
                 [-2,-1, 2,-7],
                 [ 1, 0,-3, 6]] )
\end{verbatimtab}
\normalsize

\noindent
Use the square brackets to access the matrix elements or to obtain a submatrix:



\small
\begin{verbatimtab}
>>> A[0,1]            # row 0, col 1of A 
-3
>>> A[0:2,0:3]        # top-left 2x3 submatrix of A 
[ 2, -3, -8]
[-2, -1,  2]
\end{verbatimtab}
\normalsize

\noindent
Some commonly used matrices can be created with shortcut methods:



\small
\begin{verbatimtab}
>>> eye(2)            # 2x2 identity matrix
[1, 0]
[0, 1]
>>> zeros((2, 3))
[0, 0, 0]
[0, 0, 0]
\end{verbatimtab}
\normalsize

%TODO explain matrix concatenation operations: 
%>>> M1.row_join(M2)
%[1  0  0  0  0  0  0]
%[                   ]
%[0  1  0  0  0  0  0]
%[                   ]
%[0  0  1  0  0  0  0]
%>>> M3 = zeros((4, 3))
%>>> M1.col_join(M3)


\noindent
Standard algebraic operations like 
addition \texttt{+}, subtraction \texttt{-}, multiplication \texttt{*},
and exponentiation \texttt{**} work as expected for \texttt{Matrix} objects.
%
The \texttt{transpose} operation flips the matrix through its diagonal:

\small
\begin{verbatimtab}
>>> A.transpose()    # the same as A.T
[ 2, -2,  1]
[-3, -1,  0]
[-8,  2, -3]
[ 7, -7,  6]
\end{verbatimtab}
\normalsize

\noindent
Recall that the transpose is also used to convert row vectors into column vectors and vice versa.


\subsection{Row operations}
\label{matrices:row_operations}

\small
\begin{verbatimtab}
>>> M = eye(3)
>>> M.row_op(1, lambda v,j: v+3*M[0,j] )
>>> M
[1, 0, 0]
[3, 1, 0]
[0, 0, 1]
\end{verbatimtab}
\normalsize

The method \texttt{row\_op} takes two arguments as inputs:
the first argument specifies the $0$-based index of the row you want to act on,
while the second argument is a function of the form \texttt{f(val,j)}
that describes how you want the value \texttt{val=M[i,j]} to be transformed.
The above call to \texttt{row\_op} implements the row operation $R_2 \gets R_2 + 3R_1$.
%The expression \texttt{lambda  a,b: a+b} is the \texttt{Python} syntax for creating an anonymous function with arguments
%\texttt{a} and \texttt{b}, which computes their sum \texttt{a+b}.


\subsection{Reduced row echelon form}
\label{matrices:reduced_row_echelon_form}

The Gauss--Jordan elimination procedure is a sequence of row operations you can perform
on any matrix to bring it to its \emph{reduced row echelon form} (RREF).
In \texttt{SymPy}, matrices have a \texttt{rref} method that computes their RREF: 

\small
\begin{verbatimtab}
>>> A = Matrix( [[2,-3,-8, 7],
                 [-2,-1,2,-7],
                 [1 ,0,-3, 6]])
>>> A.rref()
([1, 0, 0,  0]  # RREF of A
 [0, 1, 0,  3]                    # locations of pivots
 [0, 0, 1, -2],                   [0, 1, 2]              )
\end{verbatimtab}
\normalsize

\noindent
Note the \texttt{rref} method returns a tuple of values:
the first value is the RREF of $A$,
while the second tells you the indices of the leading ones (also known as pivots) in the RREF of $A$.
To get just the RREF of $A$, select the $0$\textsuperscript{th} entry form the tuple: \texttt{A.rref()[0]}.

%
%\small
%\begin{verbatimtab}
%>>> Arref = A.rref()[0]
%>>> Arref
%[1, 0, 0,  0]
%[0, 1, 0,  3]
%[0, 0, 1, -2]
%\end{verbatimtab}
%\normalsize



\subsection{Matrix fundamental spaces}
\label{matrices:matrix_fundamental_spaces}

Consider the matrix $A \in \mathbb{R}^{m\times n}$.
The fundamental spaces of a matrix are its column space $\mathcal{C}(A)$, 
its null space $\mathcal{N}(A)$,
and its row space $\mathcal{R}(A)$.
These vector spaces are important when you consider the matrix product
$A\vec{x}=\vec{y}$ as ``applying'' the linear transformation $T_A:\mathbb{R}^n \to \mathbb{R}^m$
to an input vector $\vec{x} \in \mathbb{R}^n$ to produce the output vector $\vec{y} \in \mathbb{R}^m$.

\textbf{Linear transformations} $T_A:\mathbb{R}^n \to \mathbb{R}^m$ (vector functions)
\textbf{are equivalent to $m\times n$ matrices}.
This is one of the fundamental ideas in linear algebra.
You can think of $T_A$ as the abstract description of the transformation 
and $A \in \mathbb{R}^{m\times n}$ as a concrete implementation of $T_A$.
By this equivalence, 
the fundamental spaces of a matrix $A$
tell us facts about the domain and image of the linear transformation $T_A$.
The columns space $\mathcal{C}(A)$ is the same as the image space space $\textrm{Im}(T_A)$ (the set of all possible outputs).
The null space $\mathcal{N}(A)$ is the same as the kernel $\textrm{Ker}(T_A)$ (the set of inputs that $T_A$ maps to the zero vector).
The row space $\mathcal{R}(A)$ is the orthogonal complement of the null space.
Input vectors in the row space of $A$ are in one-to-one correspondence with the output vectors in the column space of $A$.

Okay, enough theory! Let's see how to compute the fundamental spaces of the matrix $A$ defined above.
The non-zero rows in the reduced row echelon form of $A$ are a basis for its row space:

\small
\begin{verbatimtab}
>>> [ A.rref()[0][r,:] for r in A.rref()[1] ]       # R(A)
[  [1, 0, 0, 0],  [0, 1, 0, 3],  [0, 0, 1, -2]   ]
\end{verbatimtab}
\normalsize

\noindent
The column space of $A$ is the span of the columns of $A$ that contain the pivots
in the reduced row echelon form of $A$:



\small
\begin{verbatimtab}
>>> [ A[:,c] for c in  A.rref()[1] ]                # C(A)
[  [ 2]    [-3]   [-8]
   [-2],   [-1],  [ 2]
   [ 1]    [ 0]   [-3]  ]
\end{verbatimtab}
\normalsize

\noindent
Note we took columns from the original matrix $A$ and not its RREF.


To find the null space of $A$, call its \texttt{nullspace} method:

\small
\begin{verbatimtab}
>>> A.nullspace()                                   # N(A)
[  [0, -3, 2, 1]  ]
\end{verbatimtab}
\normalsize
\subsection{Determinants}
\label{matrices:determinants}

The determinant of a matrix, 
denoted $\det(A)$ or $|A|$, 
is a particular way to multiply the entries of the matrix to produce a single number.



\small
\begin{verbatimtab}
>>> M = Matrix( [[1, 2, 3], 
                 [2,-2, 4],
                 [2, 2, 5]] )
>>> M.det()   
2
\end{verbatimtab}
\normalsize

\noindent
Determinants are used for all kinds of tasks:
to compute areas and volumes,
to solve systems of equations, 
and to check whether a matrix is invertible or not.

\subsection{Matrix inverse}
\label{matrices:matrix_inverse}

For every invertible matrix $A$,
there exists an inverse matrix $A^{-1}$ which \emph{undoes} the effect of $A$.
The cumulative effect of the product of $A$ and $A^{-1}$ (in any order)
is the identity matrix: $AA^{-1}= A^{-1}A=\mathbbm{1}$.



\small
\begin{verbatimtab}
>>> A = Matrix( [[1,2], 
                 [3,9]] ) 
>>> A.inv()                        
[ 3, -2/3]
[-1,  1/3]
>>> A.inv()*A
[1, 0]
[0, 1]
>>> A*A.inv()
[1, 0]
[0, 1]
\end{verbatimtab}
\normalsize

\noindent
The matrix inverse $A^{-1}$ plays the role of division by $A$.


\vspace{-3mm}

\subsection{Eigenvectors and eigenvalues}
\label{matrices:eigenvectors_and_eigenvalues}

\vspace{-1mm}

		When a matrix is multiplied by one of its eigenvectors the output
		is the same eigenvector multiplied by a constant $A\vec{e}_\lambda =\lambda\vec{e}_\lambda$.
		The constant $\lambda$ (the Greek letter \emph{lambda}) is called an \emph{eigenvalue} of $A$.
		%		and the vector is called an \emph{eigenvector}.		
		%	\[ 
		%	 \Rightarrow
		%	 \quad
		%	 \left( A-\lambda \mathbbm{1}\right)	\vec{e}_\lambda 	= \vec{0}.
		%	\]
		%Thinking of matrices in term of their eigenvalues and eigenvectors is
		%a very powerful technique for describing their properties.
		%In particular 
		
		To find the eigenvalues of a matrix,  start from the definition $A\vec{e}_\lambda =\lambda\vec{e}_\lambda$,
		insert the identity $\mathbbm{1}$, 
		and rewrite it as a null-space problem:
		\[
			A\vec{e}_\lambda =\lambda\mathbbm{1}\vec{e}_\lambda
			\qquad
			\Rightarrow
			\qquad
			\left(A - \lambda\mathbbm{1}\right)\vec{e}_\lambda = \vec{0}.
		\]
		This equation will have a solution whenever $|A - \lambda\mathbbm{1}|=0$.\footnote{The invertible matrix theorem states
		that a matrix has a non-empty null space if and only if its determinant is zero.}
		%
		The eigenvalues of $A \in \mathbb{R}^{n \times n}$, 
		denoted $\{ \lambda_1, \lambda_2, \ldots, \lambda_n \}$,
		are the roots of the \emph{characteristic polynomial} $p(\lambda)=|A - \lambda \mathbbm{1}|$.
		
		

\small
\begin{verbatimtab}
>>> A = Matrix( [[ 9, -2],
                 [-2,  6]] )
>>> A.eigenvals()   # same as  solve( det(A-eye(2)*x), x)
{5: 1, 10: 1}       # eigenvalues 5 and 10 with multiplicity 1
>>> A.eigenvects()   
[(5, 1, [ 1]  
        [ 2]  ),      (10, 1, [-2]
                              [ 1]  )]
\end{verbatimtab}
\normalsize

\noindent


		Certain matrices can be written entirely in terms of their eigenvectors and their eigenvalues.
		Consider the matrix $\Lambda$ (capital Greek \emph{L}) that has the eigenvalues of the matrix $A$ on the diagonal, 
		and the matrix $Q$ constructed from the eigenvectors of~$A$ as columns:
		    \[
		     \Lambda = 
		    \scriptscriptstyle
		     \begin{bmatrix}
		     \lambda_1	&  \cdots  &  0 \\
		     \vdots 	&  \ddots  &  0  \\
		     0  	&   0      &  \lambda_n
		     \end{bmatrix}\!,
		     \ \ 
		     {\textstyle Q} \: 
		     = 
		     \begin{bmatrix}
		     \big|  &  &\Huge| \\[1.2mm]
		     \vec{e}_{\lambda_1}  & \!  \cdots \! &  \large\vec{e}_{\lambda_n} \\[1.2mm]
		     \big|  &  & \Huge| 
		     \end{bmatrix}\!,
		     \ \ 
		     {\textstyle
		     \textrm{then}
		     \ \ 
		     A =  Q\Lambda Q^{-1}.
		     }
		    \]

		Matrices that can be written this way are called \emph{diagonalizable}.		
		%The matrix $A$ can be written as the product of three matrices
		%$A=Q\Lambda Q^{-1}$.
		%This is called the \emph{eigendecomposition} of $A$.
		%The matrix $Q$ contains the eigenvectors of $A$ as columns.
		%The matrix $\Lambda$ contains the eigenvalues of $A$ on its diagonal.
To \emph{diagonalize} a matrix $A$ is to find its $Q$ and $\Lambda$ matrices:

\small
\begin{verbatimtab}
>>> Q, L = A.diagonalize()
>>> Q               # the matrix of eigenvectors 
[1,  -2]            # as columns 
[2,   1]
>>> Q.inv()
[ 1/5, 2/5]
[-2/5, 1/5]
>>> L               # the matrix of eigenvalues
[5,  0]
[0, 10]
>>> Q*L*Q.inv()     # eigendecomposition of A
[ 9, -2]
[-2,  6]
>>> Q.inv()*A*Q     # obtain L from A and Q 
[5,  0]
[0, 10]
\end{verbatimtab}
\normalsize
		
		
Not all matrices are diagonalizable.
You can check if a matrix is diagonalizable by calling its \texttt{is\_diagonalizable} method:

\small
\begin{verbatimtab}
>>> A.is_diagonalizable()
True
>>> B = Matrix( [[1, 3],
  	         [0, 1]] )
>>> B.is_diagonalizable()
False
>>> B.eigenvals()
{1: 2}              # eigenvalue 1 with multiplicity 2
>>> B.eigenvects()
[(1, 2,  [1]
         [0]  )]
\end{verbatimtab}
\normalsize

\noindent
The matrix $B$ is not diagonalizable because it doesn't have a full set of eigenvectors.
To diagonalize a $2\times 2$ matrix, we need two orthogonal eigenvectors but $B$ has only a single eigenvector.
Therefore, we can't construct the matrix of eigenvectors $Q$ (we're missing a column!) 
and so $B$ is not diagonalizable.

Non-square matrices don't have eigenvectors and therefore don't have an eigendecomposition.
Instead, we can use the \emph{singular value decomposition} to break up a non-square matrix $A$ into 
left singular vectors,
right singular vectors, 
and a diagonal matrix of singular values.
Use the \texttt{singular\_values} method on any matrix to find its singular values.

%\subsection{QR decomposition}
%\label{matrices:qr_decomposition}

%It is possible to write a matrix $A$ as the product of an orthogonal matrix $Q$
%and an upper triangular matrix $R$.
%This is known as the QR-decomposition.

%\small
%\begin{verbatimtab}
%>>> A=Matrix( [[12,-51,4], 
%               [6,167,-68], 
%               [-4,24,-41]] )
%>>> Q,R = A.QRdecomposition()
%>>> Q
%[ 6/7, -69/175, -58/175]
%[ 3/7, 158/175,   6/175]
%[-2/7,    6/35,  -33/35]
%>>> Q*Q.T                  # verify Q is orthogonal 
%[1, 0, 0]         
%[0, 1, 0]
%[0, 0, 1]  
%>>> R                      # and R is upper triangular
%[14,  21, -14]    
%[ 0, 175, -70]    
%[ 0,   0,  35]
%>>> Q*R                    # verify QR = A 
%[12, -51,   4]
%[ 6, 167, -68]
%[-4,  24, -41]
%\end{verbatimtab}
%\normalsize
%
%\noindent
%Each \texttt{sympy} matrix is also equipped with 
%\href{https://en.wikipedia.org/wiki/LU_decomposition}{\texttt{LUdecomposition}}
%and \href{https://en.wikipedia.org/wiki/Cholesky_decomposition}{\texttt{cholesky}} decomposition methods.


